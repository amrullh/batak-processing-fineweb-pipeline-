{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f36af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re  \n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from tqdm import tqdm\n",
    "import fasttext \n",
    "import os\n",
    "\n",
    "\n",
    "logger.add(\"batak_processing.log\", rotation=\"10 MB\", level=\"INFO\")\n",
    "print(\"Dependencies loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faaa7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CSV_FILE_PATH = \"clean_batak_scraped_pdfs.csv\"\n",
    "OUTPUT_CLEAN = \"batak_pdfs_CLEANED.xls\"\n",
    "PRETRAINED_MODEL_PATH = 'lid.176.ftz' \n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e035b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batak = pd.read_csv('clean_batak_scraped_pdfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6fef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_markdown_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    (VERSI V3 - SANGAT AGRESIF)\n",
    "    Membersihkan noise spesifik dari ekstraksi PDF (LaTeX, tag gambar, dll).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" \n",
    "    \n",
    "    # 1. Hapus tag gambar aneh: !(page0Picture3.jpeg)\n",
    "    text = re.sub(r'!\\(.*?\\)', '', text)\n",
    "    \n",
    "    # 2. Hapus sintaks LaTeX-like: \\sigma, \\boldsymbol{...}, \\eta\n",
    "\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+(?:\\{.*?\\})?', '', text)\n",
    "    \n",
    "    # 3. Hapus karakter Markdown/noise (#, *, _, $, [, ], `)\n",
    "    text = re.sub(r'[#\\*\\_`$\\[\\]]+', '', text) \n",
    "    \n",
    "    # 4. Hapus repeating pipes, backslashes, dan dashes: |||, ---, \\\\\n",
    "    text = re.sub(r'[\\|\\\\-]+', ' ', text) \n",
    "    \n",
    "    # 5. Ganti newline (dan tab) jadi spasi tunggal\n",
    "    text = re.sub(r'\\s*\\n\\s*|\\s*\\t\\s*', ' ', text)\n",
    "    \n",
    "    # 6. Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e55d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quality(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    - Panjang minimal 50 \n",
    "    - Filter tanda baca di akhir DIHAPUS\n",
    "    \"\"\"\n",
    "    if not text: \n",
    "        return False \n",
    "    if len(text) < 100: \n",
    "        return False \n",
    "        \n",
    "\n",
    "    if len(text) == 0:\n",
    "        return False\n",
    "    digit_ratio = sum(c.isdigit() for c in text) / len(text)\n",
    "    if digit_ratio > 0.3: \n",
    "        return False        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc1bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deteksi bahasa (fastText) dimuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import fasttext\n",
    "from loguru import logger\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        print(f\"Error: Model fastText '{PRETRAINED_MODEL_PATH}' tidak ditemukan.\")\n",
    "        lid_model = None\n",
    "    else:\n",
    "        lid_model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "        print(\"Model deteksi bahasa (fastText) dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error memuat model fastText: {e}\")\n",
    "    lid_model = None\n",
    "\n",
    "def split_text_by_language(text: str, model=lid_model):\n",
    "    \"\"\"\n",
    "    Memecah teks menjadi dua bagian: Bahasa Batak dan Bahasa Indonesia\n",
    "    berdasarkan prediksi per kalimat.\n",
    "    \"\"\"\n",
    "    if model is None or not text or not isinstance(text, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # 1. Bersihkan artifact gambar/halaman\n",
    "    text = re.sub(r'!\\(page\\d+Picture.*?\\)', '', text)\n",
    "    \n",
    "    # 2. Pecah kalimat (Split by titik)\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    batak_buffer = []\n",
    "    indo_buffer = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        # Skip kalimat terlalu pendek (kurang dari 3 kata)\n",
    "        if len(sent.split()) < 3:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # 3. Prediksi Bahasa per Kalimat\n",
    "            prediction = model.predict(sent, k=1)\n",
    "            lang_code = prediction[0][0].replace('__label__', '')\n",
    "            confidence = prediction[1][0]\n",
    "            \n",
    "            # --- LOGIKA PEMISAHAN ---\n",
    "            # Kategori INDONESIA: Terdeteksi 'id' dengan confidence > 0.5\n",
    "            if lang_code == 'id' and confidence > 0.5:\n",
    "                indo_buffer.append(sent)\n",
    "            \n",
    "            else:\n",
    "                batak_buffer.append(sent)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return \". \".join(batak_buffer), \". \".join(indo_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d6909a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near-duplicate filter function diinisiasi.\n"
     ]
    }
   ],
   "source": [
    "def filter_near_duplicates(df: pd.DataFrame, text_column: str = 'text', threshold: float = 0.90, num_perm: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Memfilter duplikat atau dokumen yang sangat mirip menggunakan MinHash LSH.\n",
    "    \"\"\"\n",
    "    print(f\"Running Near-Duplicate Cleaning (Threshold={threshold})...\")\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    minhashes = {}\n",
    "    for index, row in tqdm(df.iterrows(), total=initial_count, desc=\"1/3 Creating MinHashes\"):\n",
    "        text = str(row[text_column])\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for d in text.split(): # Tokenisasi gunakan .split \n",
    "            m.update(d.encode('utf8'))\n",
    "        minhashes[index] = m\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for index, m in tqdm(minhashes.items(), desc=\"2/3 Indexing LSH\"):\n",
    "        lsh.insert(index, m)\n",
    "\n",
    "    unique_ids = set()\n",
    "    processed_ids = set() \n",
    "    \n",
    "    for index in tqdm(df.index, desc=\"3/3 Querying Duplicates\"):\n",
    "        if index in processed_ids:\n",
    "            continue\n",
    "            \n",
    "        similar_items = set(lsh.query(minhashes[index]))\n",
    "        processed_ids.update(similar_items)\n",
    "        unique_ids.add(index)\n",
    "    df_filtered = df.loc[list(unique_ids)]\n",
    "    removed = initial_count - len(df_filtered)\n",
    "    print(f\"Done. Documents removed (duplicates): {removed}\")\n",
    "    \n",
    "    return df_filtered.copy()\n",
    "\n",
    "print(\"Near-duplicate filter function diinisiasi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c517c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_italic_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Ambil teks di antara *...* atau _..._\n",
    "    matches = re.findall(r'(\\*|_)(.*?)\\1', text)\n",
    "    return \" \".join([m[1] for m in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67d14437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model FastText berhasil dimuat.\n",
      "1. Memuat Data CSV...\n",
      "2. BERSIH-BERSIH TOTAL (ANTI HTML, TANDA SERU, KURUNG KOSONG)...\n",
      "3. Memisahkan Bahasa...\n",
      "4. Menyimpan Hasil...\n",
      "\n",
      "--- SUKSES! ---\n",
      "Batak Bersih: 47 baris -> batak_dataset_FINAL.jsonl\n",
      "Indo Bersih : 47 baris -> indonesia_dataset_FINAL.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import fasttext\n",
    "\n",
    "# --- 1. KONFIGURASI ---\n",
    "OUTPUT_BATAK = \"batak_dataset_FINAL.jsonl\"\n",
    "OUTPUT_INDO = \"indonesia_dataset_FINAL.jsonl\"\n",
    "CSV_FILE_PATH = \"clean_batak_scraped_pdfs.csv\" \n",
    "PRETRAINED_MODEL_PATH = 'lid.176.ftz'\n",
    "\n",
    "# Load Model\n",
    "lid_model = None\n",
    "try:\n",
    "    if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        lid_model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "        print(\"Model FastText berhasil dimuat.\")\n",
    "    else:\n",
    "        print(\"WARNING: Model tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# --- 2. FUNGSI CLEANING (REVISI: HAPUS TANDA SERU & KURUNG KOSONG) ---\n",
    "def cleaning_ultimate(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. HAPUS SEMUA TAG HTML/XML (<...>)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # 2. HAPUS TAG GAMBAR PDF & \"PAGE PICTURE\"\n",
    "    # Hapus pola !(...)\n",
    "    text = re.sub(r'!\\(.*?\\)', ' ', text)\n",
    "    text = re.sub(r'(?i)(page|Picture)\\s*\\d*', ' ', text)\n",
    "\n",
    "    # 3. HAPUS HEADER SKRIPSI & BIROKRASI (METADATA)\n",
    "    sampah_birokrasi = [\n",
    "        r'(?i)NIM\\s*[\\.:]?\\s*\\d+',    # NIM\n",
    "        r'(?i)NIP\\s*[\\.:]?\\s*\\d+',    # NIP\n",
    "        r'(?i)No\\.?\\s*Reg\\s*[\\.:]?\\s*\\d+', \n",
    "        r'(?i)SKRIPSI', \n",
    "        r'(?i)DISUSUN OLEH.*', \n",
    "        r'(?i)OLEH\\s*:?',\n",
    "        r'(?i)FAKULTAS\\s+\\w+',\n",
    "        r'(?i)UNIVERSITAS\\s+\\w+',\n",
    "        r'(?i)PROGRAM STUDI',\n",
    "        r'(?i)JURUSAN',\n",
    "        r'(?i)BAB\\s*[IVX]+',          \n",
    "        r'(?i)Dipindai dengan.*',     \n",
    "        r'(?i)Scanned by.*',\n",
    "        r'(?i)DAFTAR PUSTAKA',\n",
    "        r'(?i)KATA PENGANTAR',\n",
    "        r'(?i)ABSTRAK'\n",
    "    ]\n",
    "    for pola in sampah_birokrasi:\n",
    "        text = re.sub(pola, ' ', text)\n",
    "\n",
    "    # 4. HAPUS EKSTENSI FILE SISA\n",
    "    text = re.sub(r'(?i)(\\.jpeg|\\.jpg|\\.png)', ' ', text)\n",
    "\n",
    "    # 5. HAPUS TANDA BACA & SIMBOL KHUSUS [DIPERBARUI]\n",
    "    # Hapus simbol matematika LaTeX seperti $ \\mathbf $ dll\n",
    "    text = re.sub(r'\\$.*?\\$', ' ', text)  # Hapus konten dalam $...$\n",
    "    text = re.sub(r'\\\\mathbf\\s*\\{[^}]*\\}', ' ', text)  # Hapus \\mathbf{...}\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\s*\\{[^}]*\\}', ' ', text)  # Hapus perintah LaTeX lain\n",
    "    \n",
    "    # 6. HAPUS KURUNG DAN KONTENNYA [DIPERBARUI]\n",
    "    # Hapus semua ( ) beserta isinya, termasuk ( ), (text), dll\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    \n",
    "    # 7. HAPUS GARIS PANJANG & PEMISAH [DIPERBARUI]\n",
    "    text = re.sub(r'-{3,}', ' ', text)  # Hapus --- atau lebih\n",
    "    text = re.sub(r'_{3,}', ' ', text)  # Hapus ___ atau lebih\n",
    "    text = re.sub(r'={3,}', ' ', text)  # Hapus === atau lebih\n",
    "    \n",
    "    # 8. HAPUS TANDA SERU & SIMBOL LAINNYA\n",
    "    text = re.sub(r'[!\\[\\]\\{\\}\\*\\_#\\|\\\\^~<>=]+', ' ', text)\n",
    "\n",
    "    # 9. HAPUS ANGKA JOMBLO (NOMOR HALAMAN)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "\n",
    "    # 10. HAPUS SPASI BERLEBIHAN DAN KATA SINGKAT\n",
    "    # Hapus kata dengan kurang dari 2 huruf (kecuali kata penting)\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 2 or word.lower() in ['di', 'ke', 'dari', 'dan', 'atau', 'yang']:\n",
    "            filtered_words.append(word)\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    # 11. FINAL POLISH\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- 3. FUNGSI SPLIT BAHASA ---\n",
    "def split_text_by_language(text: str, model=lid_model):\n",
    "    if model is None or not text: return \"\", \"\"\n",
    "    \n",
    "    sentences = text.split('.') \n",
    "    batak_buf, indo_buf = [], []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        # Filter kalimat pendek (< 3 kata) atau kependekan (< 15 char)\n",
    "        if len(sent.split()) < 3 or len(sent) < 15: continue \n",
    "        \n",
    "        try:\n",
    "            pred = model.predict(sent, k=1)\n",
    "            lang = pred[0][0].replace('__label__', '')\n",
    "            conf = pred[1][0]\n",
    "            \n",
    "            # STRICT INDO\n",
    "            if lang == 'id' and conf > 0.6: \n",
    "                indo_buf.append(sent)\n",
    "            # BATAK (Residual)\n",
    "            elif lang not in ['ja', 'zh', 'ko', 'ru', 'ar', 'en']: \n",
    "                batak_buf.append(sent)\n",
    "        except: continue\n",
    "            \n",
    "    return \". \".join(batak_buf), \". \".join(indo_buf)\n",
    "\n",
    "# --- 4. EKSEKUSI PIPELINE ---\n",
    "try:\n",
    "    print(\"1. Memuat Data CSV...\")\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    \n",
    "    input_col = 'md_extraction_result'\n",
    "    if input_col not in df.columns:\n",
    "        input_col = df.select_dtypes(include=['object']).columns[0]\n",
    "    df.dropna(subset=[input_col], inplace=True)\n",
    "    \n",
    "    print(\"2. BERSIH-BERSIH TOTAL (ANTI HTML, TANDA SERU, KURUNG KOSONG)...\")\n",
    "    df['clean_temp'] = df[input_col].apply(cleaning_ultimate)\n",
    "    \n",
    "    print(\"3. Memisahkan Bahasa...\")\n",
    "    split_results = df['clean_temp'].apply(split_text_by_language)\n",
    "    \n",
    "    batak_list = [res[0] for res in split_results if len(res[0]) > 50]\n",
    "    indo_list = [res[1] for res in split_results if len(res[1]) > 50]\n",
    "    \n",
    "    df_batak = pd.DataFrame({'text': batak_list})\n",
    "    df_indo = pd.DataFrame({'text': indo_list})\n",
    "\n",
    "    print(\"4. Menyimpan Hasil...\")\n",
    "    df_batak.to_json(OUTPUT_BATAK, orient='records', lines=True)\n",
    "    df_indo.to_json(OUTPUT_INDO, orient='records', lines=True)\n",
    "    \n",
    "    print(f\"\\n--- SUKSES! ---\")\n",
    "    print(f\"Batak Bersih: {len(df_batak)} baris -> {OUTPUT_BATAK}\")\n",
    "    print(f\"Indo Bersih : {len(df_indo)} baris -> {OUTPUT_INDO}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCRITICAL ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587997f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a7935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2770e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
