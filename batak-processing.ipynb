{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f36af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re  \n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from tqdm import tqdm\n",
    "import fasttext \n",
    "import os\n",
    "\n",
    "\n",
    "logger.add(\"batak_processing.log\", rotation=\"10 MB\", level=\"INFO\")\n",
    "print(\"Dependencies loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faaa7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CSV_FILE_PATH = \"clean_batak_scraped_pdfs.csv\"\n",
    "OUTPUT_CLEAN = \"batak_pdfs_CLEANED.xls\"\n",
    "PRETRAINED_MODEL_PATH = 'lid.176.ftz' \n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e035b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batak = pd.read_csv('clean_batak_scraped_pdfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6fef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_markdown_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    (VERSI V3 - SANGAT AGRESIF)\n",
    "    Membersihkan noise spesifik dari ekstraksi PDF (LaTeX, tag gambar, dll).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" \n",
    "    \n",
    "    # 1. Hapus tag gambar aneh: !(page0Picture3.jpeg)\n",
    "    text = re.sub(r'!\\(.*?\\)', '', text)\n",
    "    \n",
    "    # 2. Hapus sintaks LaTeX-like: \\sigma, \\boldsymbol{...}, \\eta\n",
    "\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+(?:\\{.*?\\})?', '', text)\n",
    "    \n",
    "    # 3. Hapus karakter Markdown/noise (#, *, _, $, [, ], `)\n",
    "    text = re.sub(r'[#\\*\\_`$\\[\\]]+', '', text) \n",
    "    \n",
    "    # 4. Hapus repeating pipes, backslashes, dan dashes: |||, ---, \\\\\n",
    "    text = re.sub(r'[\\|\\\\-]+', ' ', text) \n",
    "    \n",
    "    # 5. Ganti newline (dan tab) jadi spasi tunggal\n",
    "    text = re.sub(r'\\s*\\n\\s*|\\s*\\t\\s*', ' ', text)\n",
    "    \n",
    "    # 6. Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e55d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quality(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    - Panjang minimal 50 \n",
    "    - Filter tanda baca di akhir DIHAPUS\n",
    "    \"\"\"\n",
    "    if not text: \n",
    "        return False \n",
    "    if len(text) < 100: \n",
    "        return False \n",
    "        \n",
    "\n",
    "    if len(text) == 0:\n",
    "        return False\n",
    "    digit_ratio = sum(c.isdigit() for c in text) / len(text)\n",
    "    if digit_ratio > 0.3: \n",
    "        return False        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc1bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deteksi bahasa (fastText) dimuat.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        print(f\"Error: Model fastText '{PRETRAINED_MODEL_PATH}' tidak ditemukan.\")\n",
    "        lid_model = None\n",
    "    else:\n",
    "        lid_model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "        print(\"Model deteksi bahasa (fastText) dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error memuat model fastText: {e}\")\n",
    "    lid_model = None\n",
    "\n",
    "def detect_language_indo_batak(text: str, model=lid_model, confidence_threshold=0.4):\n",
    "\n",
    "    if model is None or not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict(text, k=1)\n",
    "        lang_code = predictions[0][0].replace('__label__', '')\n",
    "        confidence = predictions[1][0]\n",
    "        \n",
    "        is_target_language = (lang_code == 'id' or lang_code == 'bts')\n",
    "        \n",
    "        return is_target_language and (confidence >= confidence_threshold)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"FastText prediction error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6909a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near-duplicate filter function diinisiasi.\n"
     ]
    }
   ],
   "source": [
    "def filter_near_duplicates(df: pd.DataFrame, text_column: str = 'text', threshold: float = 0.90, num_perm: int = 128) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Memfilter duplikat atau dokumen yang sangat mirip menggunakan MinHash LSH.\n",
    "    \"\"\"\n",
    "    print(f\"Running Near-Duplicate Cleaning (Threshold={threshold})...\")\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    minhashes = {}\n",
    "    for index, row in tqdm(df.iterrows(), total=initial_count, desc=\"1/3 Creating MinHashes\"):\n",
    "        text = str(row[text_column])\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for d in text.split(): # Tokenisasi gunakan .split \n",
    "            m.update(d.encode('utf8'))\n",
    "        minhashes[index] = m\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for index, m in tqdm(minhashes.items(), desc=\"2/3 Indexing LSH\"):\n",
    "        lsh.insert(index, m)\n",
    "\n",
    "    unique_ids = set()\n",
    "    processed_ids = set() \n",
    "    \n",
    "    for index in tqdm(df.index, desc=\"3/3 Querying Duplicates\"):\n",
    "        if index in processed_ids:\n",
    "            continue\n",
    "            \n",
    "        similar_items = set(lsh.query(minhashes[index]))\n",
    "        processed_ids.update(similar_items)\n",
    "        unique_ids.add(index)\n",
    "    df_filtered = df.loc[list(unique_ids)]\n",
    "    removed = initial_count - len(df_filtered)\n",
    "    print(f\"Done. Documents removed (duplicates): {removed}\")\n",
    "    \n",
    "    return df_filtered.copy()\n",
    "\n",
    "print(\"Near-duplicate filter function diinisiasi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d14437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total dokumen dimuat: 47\n",
      "Dokumen setelah drop NaN: 47\n",
      "\n",
      "Starting (1) Preprocessing (Markdown Clean)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-17 13:05:10.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mQuality Filtered: 0 dokumen terbuang.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai.\n",
      "\n",
      "Starting (2) Quality Filtering (Versi Santai)...\n",
      "Quality Filtered. Dokumen tersisa: 47\n",
      "\n",
      "Starting (3) Language Identification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-17 13:05:10.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLanguage Filtered: 0 dokumen (non-target) terbuang.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Filtered. Dokumen tersisa: 47\n",
      "\n",
      "Starting (4) Near-Duplicate Filtering...\n",
      "Running Near-Duplicate Cleaning (Threshold=0.95)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/3 Creating MinHashes: 100%|██████████| 47/47 [00:01<00:00, 29.99it/s]\n",
      "2/3 Indexing LSH: 100%|██████████| 47/47 [00:00<00:00, 72555.13it/s]\n",
      "3/3 Querying Duplicates: 100%|██████████| 47/47 [00:00<00:00, 96350.09it/s]\n",
      "\u001b[32m2025-11-17 13:05:11.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mNear-Duplicate Filtered: 0 dokumen terbuang.\u001b[0m\n",
      "\u001b[32m2025-11-17 13:05:11.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mProses selesai. DokMen awal: 47, Dokumen akhir: 47\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Documents removed (duplicates): 0\n",
      "Near-Duplicates Filtered. Dokumen tersisa: 47\n",
      "\n",
      "Starting (5) Saving cleaned data...\n",
      "\n",
      "--- PROSES SELESAI ---\n",
      "Dokumen awal: 47\n",
      "Dokumen akhir: 47.\n",
      "Data bersih disimpan di: batak_pdfs_CLEANED.xls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    df_batak = pd.read_csv(CSV_FILE_PATH)\n",
    "    initial_doc_count = len(df_batak)\n",
    "    print(f\"\\nTotal dokumen dimuat: {initial_doc_count}\")\n",
    "\n",
    "    INPUT_COLUMN = 'md_extraction_result'\n",
    "    CLEAN_COLUMN = 'text_clean'\n",
    "\n",
    "    df_batak.dropna(subset=[INPUT_COLUMN], inplace=True)\n",
    "    print(f\"Dokumen setelah drop NaN: {len(df_batak)}\")\n",
    "\n",
    "    # 1. Preprocessing (Membersihkan Markdown) \n",
    "    print(\"\\nStarting (1) Preprocessing (Markdown Clean)...\")\n",
    "    df_batak[CLEAN_COLUMN] = df_batak[INPUT_COLUMN].apply(clean_markdown_text)\n",
    "    print(\"Selesai.\")\n",
    "\n",
    "    # 2. Quality Filtering  \n",
    "    print(\"\\nStarting (2) Quality Filtering (Versi Santai)...\")\n",
    "    df_batak['is_quality_safe'] = df_batak[CLEAN_COLUMN].apply(check_quality)\n",
    "    df_clean = df_batak[df_batak['is_quality_safe'] == True].copy()\n",
    "    print(f\"Quality Filtered. Dokumen tersisa: {len(df_clean)}\")\n",
    "    logger.info(f\"Quality Filtered: {len(df_batak) - len(df_clean)} dokumen terbuang.\")\n",
    "\n",
    "    # 3. Language Identification (Filter 'id' + 'bts') \n",
    "    print(\"\\nStarting (3) Language Identification...\")\n",
    "    if lid_model is not None:\n",
    "        df_clean['is_target_lang'] = df_clean[CLEAN_COLUMN].apply(detect_language_indo_batak)\n",
    "        df_lang_filtered = df_clean[df_clean['is_target_lang'] == True].copy()\n",
    "        print(f\"Language Filtered. Dokumen tersisa: {len(df_lang_filtered)}\")\n",
    "        logger.info(f\"Language Filtered: {len(df_clean) - len(df_lang_filtered)} dokumen (non-target) terbuang.\")\n",
    "    else:\n",
    "        print(\"Model fastText tidak dimuat, melewati langkah Language Filter.\")\n",
    "        df_lang_filtered = df_clean.copy() \n",
    "\n",
    "    # 4. Near-Duplicate Filtering\n",
    "    print(\"\\nStarting (4) Near-Duplicate Filtering...\")\n",
    "    df_lang_filtered.reset_index(drop=True, inplace=True) \n",
    "    df_final = filter_near_duplicates(df_lang_filtered, text_column=CLEAN_COLUMN, threshold=0.95, num_perm=128)\n",
    "    print(f\"Near-Duplicates Filtered. Dokumen tersisa: {len(df_final)}\")\n",
    "    logger.info(f\"Near-Duplicate Filtered: {len(df_lang_filtered) - len(df_final)} dokumen terbuang.\")\n",
    "\n",
    "    # 5. Penyimpanan Hasil Bersih \n",
    "    print(\"\\nStarting (5) Saving cleaned data...\")\n",
    "    final_columns = [CLEAN_COLUMN, INPUT_COLUMN, 'extracted_meaningful_text_v2']\n",
    "    cols_to_save = [col for col in final_columns if col in df_final.columns]\n",
    "    \n",
    "    df_final[cols_to_save].to_json(OUTPUT_CLEAN, orient='records', lines=True)\n",
    "\n",
    "    final_doc_count = len(df_final)\n",
    "    print(f\"\\n--- PROSES SELESAI ---\")\n",
    "    print(f\"Dokumen awal: {initial_doc_count}\")\n",
    "    print(f\"Dokumen akhir: {final_doc_count}.\")\n",
    "    print(f\"Data bersih disimpan di: {OUTPUT_CLEAN}\")\n",
    "    logger.info(f\"Proses selesai. DokMen awal: {initial_doc_count}, Dokumen akhir: {final_doc_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- !!! ERROR DI TENGAH JALAN !!! ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    logger.error(f\"Pipeline gagal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587997f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a7935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
